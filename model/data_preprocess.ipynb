{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess for US Wheat Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from fredapi import Fred\n",
    "\n",
    "fred = Fred(api_key='2110034933792184d022f67f36bd7295')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_STATION_MAP = {\n",
    "    'ALABAMA': '72226',\n",
    "    'ARIZONA': '72278',\n",
    "    'ARKANSAS': '72340',\n",
    "    'CALIFORNIA': '72390',\n",
    "    'COLORADO': '72466',\n",
    "    'CONNECTICUT': '72504',\n",
    "    'DELAWARE': '72418',\n",
    "    'FLORIDA': '120001',\n",
    "    'GEORGIA': '130001',\n",
    "    'IDAHO': '160001',\n",
    "    'ILLINOIS': '170001',\n",
    "    'INDIANA': '180001',\n",
    "    'IOWA': '190001',\n",
    "    'KANSAS': '200001',\n",
    "    'KENTUCKY': '210001',\n",
    "    'LOUISIANA': '220001',\n",
    "    'MAINE': '230001',\n",
    "    'MARYLAND': '240001',\n",
    "    'MASSACHUSETTS': '250001',\n",
    "    'MICHIGAN': '260001',\n",
    "    'MINNESOTA': '270001',\n",
    "    'MISSISSIPPI': '280001',\n",
    "    'MISSOURI': '290001',\n",
    "    'MONTANA': '300001',\n",
    "    'NEBRASKA': '310001',\n",
    "    'NEVADA': '320001',\n",
    "    'NEW HAMPSHIRE': '330001',\n",
    "    'NEW JERSEY': '340001',\n",
    "    'NEW MEXICO': '350001',\n",
    "    'NEW YORK': '360001',\n",
    "    'NORTH CAROLINA': '370001',\n",
    "    'NORTH DAKOTA': '380001',\n",
    "    'OHIO': '390001',\n",
    "    'OKLAHOMA': '400001',\n",
    "    'OREGON': '410001',\n",
    "    'PENNSYLVANIA': '420001',\n",
    "    'RHODE ISLAND': '440001',\n",
    "    'SOUTH CAROLINA': '450001',\n",
    "    'SOUTH DAKOTA': '460001',\n",
    "    'TENNESSEE': '470001',\n",
    "    'TEXAS': '480001',\n",
    "    'UTAH': '490001',\n",
    "    'VERMONT': '500001',\n",
    "    'VIRGINIA': '510001',\n",
    "    'WASHINGTON': '530001',\n",
    "    'WEST VIRGINIA': '540001',\n",
    "    'WISCONSIN': '550001',\n",
    "    'WYOMING': '560001',\n",
    "}\n",
    "\n",
    "STATE_CODE_MAP = {\n",
    "    'ALABAMA': 'AL',\n",
    "    'ARIZONA': 'AZ',\n",
    "    'ARKANSAS': 'AR',\n",
    "    'CALIFORNIA': 'CA',\n",
    "    'COLORADO': 'CO',\n",
    "    'CONNECTICUT': 'CT',\n",
    "    'DELAWARE': 'DE',\n",
    "    'FLORIDA': 'FL',\n",
    "    'GEORGIA': 'GA',\n",
    "    'IDAHO': 'ID',\n",
    "    'ILLINOIS': 'IL',\n",
    "    'INDIANA': 'IN',\n",
    "    'IOWA': 'IA',\n",
    "    'KANSAS': 'KS',\n",
    "    'KENTUCKY': 'KY',\n",
    "    'LOUISIANA': 'LA',\n",
    "    'MAINE': 'ME',\n",
    "    'MARYLAND': 'MD',\n",
    "    'MASSACHUSETTS': 'MA',\n",
    "    'MICHIGAN': 'MI',\n",
    "    'MINNESOTA': 'MN',\n",
    "    'MISSISSIPPI': 'MS',\n",
    "    'MISSOURI': 'MO',\n",
    "    'MONTANA': 'MT',\n",
    "    'NEBRASKA': 'NE',\n",
    "    'NEVADA': 'NV',\n",
    "    'NEW HAMPSHIRE': 'NH',\n",
    "    'NEW JERSEY': 'NJ',\n",
    "    'NEW MEXICO': 'NM',\n",
    "    'NEW YORK': 'NY',\n",
    "    'NORTH CAROLINA': 'NC',\n",
    "    'NORTH DAKOTA': 'ND',\n",
    "    'OHIO': 'OH',\n",
    "    'OKLAHOMA': 'OK',\n",
    "    'OREGON': 'OR',\n",
    "    'PENNSYLVANIA': 'PA',\n",
    "    'RHODE ISLAND': 'RI',\n",
    "    'SOUTH CAROLINA': 'SC',\n",
    "    'SOUTH DAKOTA': 'SD',\n",
    "    'TENNESSEE': 'TN',\n",
    "    'TEXAS': 'TX',\n",
    "    'UTAH': 'UT',\n",
    "    'VERMONT': 'VT',\n",
    "    'VIRGINIA': 'VA',\n",
    "    'WASHINGTON': 'WA',\n",
    "    'WEST VIRGINIA': 'WV',\n",
    "    'WISCONSIN': 'WI',\n",
    "    'WYOMING': 'WY'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore first row of the excel file\n",
    "df = pd.read_excel(\"data/open-data-for-dot-density-map.xlsx\", skiprows=1, dtype=str)\n",
    "\n",
    "# remove the last row of the dataframe\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "# print the last 5 rows of the dataframe\n",
    "# # print the shape of the dataframe\n",
    "# df.shape\n",
    "# # print the columns of the dataframe\n",
    "# df.columns\n",
    "# # print the data types of the dataframe\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Coordinates from the state county codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the last 5 rows of the dataframe\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create wheat-state proportion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter class to all wheat\n",
    "df_all_wheat = df[df['Class'] == 'All wheat'].copy()\n",
    "# 2. Rename all the columns: State -> state, Class -> class, 2024 planted area (acres) -> planted_area\n",
    "df_all_wheat.rename(columns={'State': 'state', 'Class': 'class', '2024 planted area (acres)': 'planted_area'}, inplace=True)\n",
    "\n",
    "# 3. Uppercase the state column\n",
    "df_all_wheat['state'] = df_all_wheat['state'].str.upper()\n",
    "# 4. Make planted_area a float\n",
    "df_all_wheat['planted_area'] = df_all_wheat['planted_area'].astype(float)\n",
    "# 5. Group by State, aggregate the planted area\n",
    "df_all_wheat_state = df_all_wheat.groupby('state').agg({'planted_area': 'sum'}).reset_index()\n",
    "df_all_wheat_state['weight'] = df_all_wheat_state['planted_area'] / df_all_wheat_state['planted_area'].sum()\n",
    "# print the last 5 rows of the dataframe\n",
    "# print(df_all_wheat_state)\n",
    "# 6. Save the dataframe to a csv file\n",
    "df_all_wheat_state.to_csv('data/all_wheat_state_proportion.csv', index=False)\n",
    "# print(df_all_wheat_state.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the station ID from each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "ALABAMA: 72223 - Mobile Regional Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "ARIZONA: 72274 - Tucson International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "ARKANSAS: 72340 - North Little Rock, North Little Rock Airport\n",
      "                                       name country region    wmo  icao  \\\n",
      "id                                                                        \n",
      "72281         El Centro, Naval Air Facility      US     CA  72281  KNJK   \n",
      "72286      Riverside / March Air Force Base      US     CA  72286  KRIV   \n",
      "72288             Burbank-Glendale-Pasadena      US     CA  72288  KBUR   \n",
      "72290       San Diego International Airport      US     CA  72290  KSAN   \n",
      "72292               Avalon Catalina Airport      US     CA  72292  KAVX   \n",
      "72295                   Los Angeles Airport      US     CA  72295  KLAX   \n",
      "72297                    Long Beach Airport      US     CA  72297  KLGB   \n",
      "72382            Palmdale Prod Flight Plant      US     CA  72382  KPMD   \n",
      "72383                              Sandberg      US     CA  72383  KSDB   \n",
      "72384             Bakersfield Meadows Field      US     CA  72384  KBFL   \n",
      "72389                   Fresno Air Terminal      US     CA  72389  KFAT   \n",
      "72391               Nawcwpns Point Mugu, Ca      US     CA  72391  KNTD   \n",
      "72392             Santa Barbara / La Patera      US     CA  72392  KSBA   \n",
      "72394            Santa Maria Public Airport      US     CA  72394  KSMX   \n",
      "72480                        Bishop Airport      US     CA  72480  KBIH   \n",
      "72483          Sacramento Executive Airport      US     CA  72483  KSAC   \n",
      "72492         Stockton Metropolitan Airport      US     CA  72492  KSCK   \n",
      "72493  Metro Oakland International  Airport      US     CA  72493  KOAK   \n",
      "72494                 San Francisco Airport      US     CA  72494  KSFO   \n",
      "72495                       Farallon Island      US     CA  72495  K50Q   \n",
      "72585                Hayward / Russell City      US     CA  72585  KHWD   \n",
      "72591           Red Bluff Municipal Airport      US     CA  72591  KRBL   \n",
      "72592             Redding Municipal Airport      US     CA  72592  KRDD   \n",
      "72593          Salinas / Spreckels Junction      US     CA  72593  KSNS   \n",
      "72594                                Eureka      US     CA  72594  KEKA   \n",
      "74612                       Naws China Lake      US     CA  74612  KNID   \n",
      "74702             Lemoore Naval Air Station      US     CA  74702  KNLC   \n",
      "U9ANI                            Julian CDF      US     CA   <NA>  <NA>   \n",
      "\n",
      "       latitude  longitude  elevation             timezone hourly_start  \\\n",
      "id                                                                        \n",
      "72281   32.8167  -115.6500      -13.0  America/Los_Angeles   1984-04-13   \n",
      "72286   33.9000  -117.2500      469.0  America/Los_Angeles   1933-01-01   \n",
      "72288   34.2000  -118.3667      236.0  America/Los_Angeles   1943-06-01   \n",
      "72290   32.7333  -117.1833        5.0  America/Los_Angeles   1942-01-01   \n",
      "72292   33.4000  -118.4167      488.0  America/Los_Angeles   1943-06-12   \n",
      "72295   33.9333  -118.3833       38.0  America/Los_Angeles   1944-01-01   \n",
      "72297   33.8167  -118.1500       17.0  America/Los_Angeles   1943-01-01   \n",
      "72382   34.6333  -118.0833      775.0  America/Los_Angeles   1973-01-01   \n",
      "72383   34.7500  -118.7167     1379.0  America/Los_Angeles   1973-01-01   \n",
      "72384   35.4333  -119.0500      155.0  America/Los_Angeles   1941-10-01   \n",
      "72389   36.7833  -119.7167      101.0  America/Los_Angeles   1941-12-04   \n",
      "72391   34.1167  -119.1167        4.0  America/Los_Angeles   1973-01-01   \n",
      "72392   34.4262  -119.8415        4.0  America/Los_Angeles   1973-01-01   \n",
      "72394   34.9000  -120.4667       79.0  America/Los_Angeles   1973-01-01   \n",
      "72480   37.3667  -118.3667     1256.0  America/Los_Angeles   1943-01-16   \n",
      "72483   38.5000  -121.5000        6.0  America/Los_Angeles   1973-01-01   \n",
      "72492   37.9000  -121.2333        9.0  America/Los_Angeles   1941-01-03   \n",
      "72493   37.7167  -122.2333        2.0  America/Los_Angeles   1943-01-01   \n",
      "72494   37.6167  -122.3667        3.0  America/Los_Angeles   1973-01-01   \n",
      "72495   37.7000  -123.0000       12.0  America/Los_Angeles          NaT   \n",
      "72585   37.6589  -122.1218       16.0  America/Los_Angeles   2000-01-01   \n",
      "72591   40.1500  -122.2500      106.0  America/Los_Angeles   1973-01-01   \n",
      "72592   40.5000  -122.2833      153.0  America/Los_Angeles   1992-01-01   \n",
      "72593   36.6628  -121.6064       26.0  America/Los_Angeles   1941-08-05   \n",
      "72594   40.8000  -124.1667       13.0  America/Los_Angeles   2006-01-01   \n",
      "74612   35.6833  -117.7000      696.0  America/Los_Angeles   1973-01-01   \n",
      "74702   36.3333  -119.9500       71.0  America/Los_Angeles   1961-07-01   \n",
      "U9ANI   33.0833  -116.6300     1285.0  America/Los_Angeles          NaT   \n",
      "\n",
      "      hourly_end daily_start  daily_end monthly_start monthly_end  \n",
      "id                                                                 \n",
      "72281 2025-06-19  1945-02-01 2025-06-04    1945-01-01  2022-01-01  \n",
      "72286 2025-06-19  1936-02-16 2025-06-09    1936-01-01  2022-01-01  \n",
      "72288 2025-06-19  1943-06-02 2025-06-09    1943-01-01  2022-01-01  \n",
      "72290 2025-06-19  1939-07-01 2025-06-14    1939-01-01  2022-01-01  \n",
      "72292 2025-06-19  1943-06-13 2025-06-14    1943-01-01  2022-01-01  \n",
      "72295 2025-06-19  1944-01-01 2025-06-14    1944-01-01  2022-01-01  \n",
      "72297 2025-06-19  1943-01-01 2025-06-14    1943-01-01  2022-01-01  \n",
      "72382 2025-06-19  1934-01-02 2025-06-14    1934-01-01  2022-01-01  \n",
      "72383 2025-06-19  1934-07-01 2025-06-12    1934-01-01  2022-01-01  \n",
      "72384 2025-06-19  1937-10-01 2025-06-14    1937-01-01  2022-01-01  \n",
      "72389 2025-06-19  1941-12-04 2025-06-14    1942-01-01  2022-01-01  \n",
      "72391 2025-06-19  1946-03-01 2025-06-10    1946-01-01  2022-01-01  \n",
      "72392 2025-06-19  1941-01-02 2025-06-14    1941-01-01  2022-01-01  \n",
      "72394 2025-06-19  1948-01-01 2025-06-14    1948-01-01  2022-01-01  \n",
      "72480 2025-06-19  1895-01-04 2025-06-14    1895-01-01  2022-01-01  \n",
      "72483 2025-06-19  1941-11-10 2025-06-14    1941-01-01  2022-01-01  \n",
      "72492 2025-06-19  1941-02-20 2025-06-14    1941-01-01  2022-01-01  \n",
      "72493 2025-06-19  1943-01-02 2025-06-14    1943-01-01  2022-01-01  \n",
      "72494 2025-06-19  1945-07-01 2025-06-14    1945-01-01  2022-01-01  \n",
      "72495        NaT  1999-06-06 2025-06-14    1999-01-01  2022-01-01  \n",
      "72585 2025-06-19  1998-09-19 2025-06-14    1998-01-01  2022-01-01  \n",
      "72591 2025-06-19  1892-01-01 2025-06-14    1892-01-01  2022-01-01  \n",
      "72592 2025-06-19  1986-09-01 2025-06-14    1986-01-01  2022-01-01  \n",
      "72593 2025-06-19  1930-06-14 2025-06-14    1930-01-01  2022-01-01  \n",
      "72594 2025-05-01  1941-12-01 2025-06-14    1941-01-01  2022-01-01  \n",
      "74612 2025-06-19  1944-02-01 2025-05-05    1944-01-01  2022-01-01  \n",
      "74702 2025-06-19  1961-07-01 2025-06-14    1961-01-01  2022-01-01  \n",
      "U9ANI        NaT  1893-01-01 2025-04-30    1893-01-01  2022-01-01  \n",
      "CALIFORNIA: 72281 - El Centro, Naval Air Facility\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "COLORADO: 72462 - San Luis Valley Regional\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "CONNECTICUT: 72504 - Sikorsky Memorial Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "DELAWARE: 72418 - Wilmington / Duross Heights\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "FLORIDA: 72201 - Key West Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "GEORGIA: 72207 - Savannah Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "IDAHO: 72578 - Pocatello Regional Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "ILLINOIS: 72433 - Salem, Salem-Leckrone Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "INDIANA: 72432 - Evansville Regional\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "IOWA: 72542 - Burlington / Congers Mobile Home Park\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "KANSAS: 72447 - Olathe / Clare\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "KENTUCKY: 72421 - Cincinnati/Northern Ky International\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "LOUISIANA: 72231 - New Orleans Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MAINE: 72606 - Portland International  Jetport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MARYLAND: 72398 - Salisbury / Quail Ridge\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MASSACHUSETTS: 72509 - Boston Logan International\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MICHIGAN: 72537 - Detroit Metropolitan\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MINNESOTA: 72644 - Rochester International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MISSISSIPPI: 72234 - Meridian Key Field\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MISSOURI: 72330 - Poplar Bluff / Green Forest\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "MONTANA: 72677 - Logan International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEBRASKA: 72550 - Omaha Eppley Airfield\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEVADA: 72386 - McCarran International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEW HAMPSHIRE: 72605 - Concord, Concord Municipal Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEW JERSEY: 72407 - Atlantic City International\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEW MEXICO: 72268 - Roswell Industrial Air Centr\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NEW YORK: 72503 - LaGuardia Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NORTH CAROLINA: 72301 - Hickory / Longview\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "NORTH DAKOTA: 72753 - Hector International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "OHIO: 72420 - Mansfield / Amoy\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "OKLAHOMA: 72353 - Will Rogers World Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "OREGON: 72597 - Rogue Valley International  Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "PENNSYLVANIA: 72399 - Harrisburg\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "RHODE ISLAND: 72507 - Providence Green State Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "SOUTH CAROLINA: 72208 - Charleston Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "SOUTH DAKOTA: 72651 - Sioux Falls Foss Field\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "TENNESSEE: 72324 - Lovell Field\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "TEXAS: 72241 - Southeast Texas Rgnl Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "UTAH: 72470 - Price Carbon County Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "VERMONT: 72614 - St Johnsbury Fairbanks\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "VIRGINIA: 72308 - Norfolk International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "WASHINGTON: 72781 - Yakima Air Terminal\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "WEST VIRGINIA: 72412 - Raleigh County Mem Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "WISCONSIN: 72640 - Gen Mitchell International Airport\n",
      "Empty DataFrame\n",
      "Columns: [name, country, region, wmo, icao, latitude, longitude, elevation, timezone, hourly_start, hourly_end, daily_start, daily_end, monthly_start, monthly_end]\n",
      "Index: []\n",
      "WYOMING: 72564 - Cheyenne Airport\n"
     ]
    }
   ],
   "source": [
    "from meteostat import Point, Daily, Stations\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2025, 5, 1)\n",
    "\n",
    "station_list = []\n",
    "\n",
    "for state, sc in STATE_CODE_MAP.items():\n",
    "\n",
    "    try:\n",
    "        # Fetch all stations in US with coverage in the date range\n",
    "        stations = Stations().region('US').inventory('daily', (start, end)).fetch()\n",
    "        \n",
    "        # Filter to just this state's stations\n",
    "        stations_state = stations[stations['region'] == sc]\n",
    "\n",
    "        if not stations_state.empty:\n",
    "            # provide the first the station_id and station_name for the state\n",
    "            # print all the station state and station_id in california\n",
    "            print(stations_state[stations_state['region'] == 'CA'])\n",
    "            station_id = stations_state.index[0]\n",
    "            station_name = stations_state.loc[station_id, 'name']\n",
    "            print(f\"{state}: {station_id} - {station_name}\")\n",
    "            station_list.append({'state': state, 'station_id': station_id, 'station_name': station_name})\n",
    "        else:\n",
    "            print(f\"{state}: No station with daily data in specified range.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{state}: Error - {e}\")\n",
    "\n",
    "station_df = pd.DataFrame(station_list)\n",
    "station_df.to_csv('data/station_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate weather data from 2000 to 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the precipitation data from 2000 to 2025 for each state\n",
    "# and based on the planted area, calculate weighted precipitation and tavg from each station, and aggregate them to combine to each date\n",
    "\n",
    "# create a column with date, from 2000 to 2025\n",
    "weather_records = []\n",
    "\n",
    "for _, row in station_df.iterrows():\n",
    "    state = row['state']\n",
    "    station_id = row['station_id']\n",
    "    weight = df_all_wheat_state.loc[df_all_wheat_state['state'] == state, 'weight'].values[0]\n",
    "\n",
    "    data = Daily(station_id, start, end).fetch()\n",
    "    if not data.empty:\n",
    "        data = data[['tavg', 'prcp']].copy()\n",
    "        data['date'] = data.index\n",
    "        data['state'] = state\n",
    "        data['weight'] = weight\n",
    "        weather_records.append(data)\n",
    "\n",
    "\n",
    "weather_df = pd.concat(weather_records)\n",
    "\n",
    "daily_weighted = weather_df.groupby('date').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'weighted_tavg': (g['tavg'].fillna(0) * g['weight']).sum(),\n",
    "        'weighted_prcp': (g['prcp'].fillna(0) * g['weight']).sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# print the last 5 rows of the dataframe\n",
    "# print(daily_weighted.tail())\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "daily_weighted.to_csv('data/daily_weighted.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_weighted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdaily_weighted\u001b[49m.head())\n",
      "\u001b[31mNameError\u001b[39m: name 'daily_weighted' is not defined"
     ]
    }
   ],
   "source": [
    "print(daily_weighted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "\n",
    "# # Filter the original DataFrame to keep only \"All wheat\"\n",
    "# df_filtered = df[df['class'] == 'All wheat'].copy()\n",
    "# # 1. Load county shapefile from Census\n",
    "# gdf = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_county_5m.zip\")\n",
    "\n",
    "# # 2. Reproject to a projected CRS (EPSG:3857 for centroid accuracy)\n",
    "# gdf_proj = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# # 3. Compute projected centroids\n",
    "# gdf_proj['centroid_proj'] = gdf_proj.geometry.centroid\n",
    "\n",
    "# # 4. Convert centroids back to lat/lon (EPSG:4326)\n",
    "# gdf_proj = gdf_proj.set_geometry('centroid_proj').to_crs(epsg=4326)\n",
    "# gdf_proj['centroid_lon'] = gdf_proj.geometry.x\n",
    "# gdf_proj['centroid_lat'] = gdf_proj.geometry.y\n",
    "\n",
    "# # 5. Merge with your DataFrame on state and county code\n",
    "# df_with_coords = df_filtered.merge(\n",
    "#     gdf_proj[['STATEFP', 'COUNTYFP', 'centroid_lat', 'centroid_lon']],\n",
    "#     left_on=['State code', 'County code'],\n",
    "#     right_on=['STATEFP', 'COUNTYFP'],\n",
    "#     how='left'\n",
    "# )\n",
    "\n",
    "# df_with_coords = df_with_coords[['State code', 'County code', 'centroid_lat', 'centroid_lon', '2024 planted area (acres)']].drop_duplicates().reset_index(drop=True).sort_values(by=['State code', 'County code'])\n",
    "# df_with_coords['2024 planted area (acres)'] = df_with_coords['2024 planted area (acres)'].astype(float)\n",
    "\n",
    "# # weight of the area planted per county\n",
    "# df_with_coords['weight'] = df_with_coords['2024 planted area (acres)'] / df_with_coords['2024 planted area (acres)'].sum()\n",
    "\n",
    "# # 6. Final result\n",
    "# print(df_with_coords)\n",
    "\n",
    "# # save the dataframe to a csv file\n",
    "# df_with_coords.to_csv('data/county_coords.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example retrieval\n",
    "from datetime import datetime, timedelta\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Define location (latitude, longitude, elevation in meters)\n",
    "location = Point(40.7128, -74.0060)  # Example: New York City\n",
    "\n",
    "# Define date range\n",
    "start = datetime(2020, 1, 1)\n",
    "end = datetime(2020, 12, 31)\n",
    "\n",
    "# Fetch daily data\n",
    "data = Daily(location, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define location (latitude, longitude, elevation in meters)\n",
    "location = Point(40.7128, -74.0060)  # Example: New York City\n",
    "\n",
    "# Define date range\n",
    "start = datetime(2020, 1, 1)\n",
    "end = datetime(2020, 12, 31)\n",
    "\n",
    "# Fetch daily data\n",
    "data = Daily(location, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve data from meteostat\n",
    "# import pandas as pd\n",
    "# from meteostat import Point, Daily, Stations\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Load your full dataset (replace with your actual CSV or DataFrame)\n",
    "# # df = pd.read_csv(\"your_county_data.csv\")  # or use your in-memory DataFrame\n",
    "\n",
    "# # Filter out counties with zero planted area\n",
    "# df_with_weight = df_with_coords[df_with_coords['2024 planted area (acres)'] > 0].copy()\n",
    "\n",
    "# # Normalize weight\n",
    "# # df_with_weight['weight'] = df_with_weight['2024 planted area (acres)'] / df_with_weight['2024 planted area (acres)'].sum()\n",
    "\n",
    "# # Date range\n",
    "# # start = datetime(2000, 1, 1)\n",
    "# # start = datetime(2025, 6, 1)\n",
    "# end = datetime.today() - timedelta(days=5)\n",
    "# # past 20 years\n",
    "# start = end - timedelta(days=7300)  # Past 30 days\n",
    "\n",
    "# end = datetime.now()\n",
    "\n",
    "# # Collect weather data\n",
    "# weather_records = []\n",
    "\n",
    "# for _, row in df_with_weight.iterrows():\n",
    "#     try:\n",
    "#         lat = row['centroid_lat']\n",
    "#         lon = row['centroid_lon']\n",
    "#         state = row['State code']\n",
    "#         county = row['County code']\n",
    "\n",
    "#         # Filter for stations with daily data in the desired date range\n",
    "#         stations = Stations().nearby(lat, lon).fetch(10)  # Get top 5 nearby stations\n",
    "\n",
    "#         if stations.empty:\n",
    "#             print(f\"No stations found for {state}-{county}\")\n",
    "#             continue\n",
    "\n",
    "#         # Try each station until one returns valid weather data\n",
    "#         weather = None\n",
    "#         for station_id in stations.index:\n",
    "#             data = Daily(station_id, start, end).fetch()\n",
    "#             if not data.empty:\n",
    "#                 weather = data\n",
    "#                 break  # Stop when we get valid data\n",
    "\n",
    "#         if weather is None:\n",
    "#             print(f\"No weather data available for any nearby station for {state}-{county}\")\n",
    "#             continue\n",
    "\n",
    "#         # Process the weather data\n",
    "#         weather = weather[['tavg', 'prcp']].copy()\n",
    "#         weather['date'] = weather.index\n",
    "#         weather['state_code'] = state\n",
    "#         weather['county_code'] = county\n",
    "#         weather['weight'] = row['weight']\n",
    "#         weather_records.append(weather)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {row['State code']}-{row['County code']}: {e}\")\n",
    "\n",
    "# # Combine all weather data\n",
    "# weather_df = pd.concat(weather_records)\n",
    "\n",
    "# # Compute weighted average per date\n",
    "# daily_weighted = weather_df.groupby('date').apply(\n",
    "#     lambda g: pd.Series({\n",
    "#         'weighted_tavg': (g['tavg'] * g['weight']).sum(),\n",
    "#         'weighted_prcp': (g['prcp'] * g['weight']).sum()\n",
    "#     })\n",
    "# ).reset_index()\n",
    "\n",
    "# # Save or use as needed\n",
    "# daily_weighted.to_csv(\"data/weighted_weather_2000_2025.csv\", index=False)\n",
    "# print(daily_weighted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from yahoo finance (source = CME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/29rzgzxj7fvdl0qwf0ybn7_00000gn/T/ipykernel_37923/382012258.py:4: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  wheat_data = yf.download('ZW=F', start='2000-01-01', end='2025-05-30')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Get CME wheat data from 2000 to 2025\n",
    "wheat_data = yf.download('ZW=F', start='2000-01-01', end='2025-05-30')\n",
    "\n",
    "# print the last 5 rows of the dataframe\n",
    "wheat_data.reset_index(inplace=True)\n",
    "# print(wheat_data.tail())\n",
    "# If MultiIndex columns exist, flatten them\n",
    "if isinstance(wheat_data.columns, pd.MultiIndex):\n",
    "    wheat_data.columns = wheat_data.columns.get_level_values(0)\n",
    "\n",
    "# Reset index and rename for merging\n",
    "# wheat_data = wheat_data.reset_index()[['Date', 'Close']]\n",
    "# wheat_data = wheat_data.rename(columns={'Date': 'date', 'Close': 'wheat_price'})\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "wheat_data.to_csv('data/wheat_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>542.50</td>\n",
       "      <td>548.50</td>\n",
       "      <td>538.50</td>\n",
       "      <td>545.0</td>\n",
       "      <td>61665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2025-05-27</td>\n",
       "      <td>528.50</td>\n",
       "      <td>544.00</td>\n",
       "      <td>527.50</td>\n",
       "      <td>543.5</td>\n",
       "      <td>66898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>530.25</td>\n",
       "      <td>535.75</td>\n",
       "      <td>526.75</td>\n",
       "      <td>531.0</td>\n",
       "      <td>60152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>534.00</td>\n",
       "      <td>534.75</td>\n",
       "      <td>527.25</td>\n",
       "      <td>533.0</td>\n",
       "      <td>48247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>534.00</td>\n",
       "      <td>534.75</td>\n",
       "      <td>527.25</td>\n",
       "      <td>533.0</td>\n",
       "      <td>48247.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   close    high     low   open   volume\n",
       "9277 2025-05-26  542.50  548.50  538.50  545.0  61665.0\n",
       "9278 2025-05-27  528.50  544.00  527.50  543.5  66898.0\n",
       "9279 2025-05-28  530.25  535.75  526.75  531.0  60152.0\n",
       "9280 2025-05-29  534.00  534.75  527.25  533.0  48247.0\n",
       "9281 2025-05-30  534.00  534.75  527.25  533.0  48247.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge weather data and wheat price data\n",
    "wheat_data = pd.read_csv('data/wheat_data.csv')\n",
    "# wheat_data['date'] = pd.to_datetime(wheat_data['date'])\n",
    "wheat_data.rename(columns={'Date': 'date', 'Close': 'close', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "full_dates = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2000-01-01', end='2025-05-30')\n",
    "})\n",
    "wheat_data['date'] = pd.to_datetime(wheat_data['date'])\n",
    "# drop the Date column\n",
    "# wheat_data = wheat_data.drop(columns=['Date'])\n",
    "wheat_data_full = pd.merge(full_dates, wheat_data, on='date', how='left')\n",
    "wheat_data_full.tail()\n",
    "wheat_data_full = wheat_data_full.ffill()\n",
    "wheat_data_full.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_weighted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m daily_weighted[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdaily_weighted\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# merge the two dataframes on the date column\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# asof merge\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# merged_data = pd.merge(wheat_data_full, daily_weighted, on='date', how='left')\u001b[39;00m\n\u001b[32m      6\u001b[39m merged_data = pd.merge_asof(wheat_data_full, daily_weighted, on=\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m, direction=\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'daily_weighted' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "daily_weighted['date'] = pd.to_datetime(daily_weighted['date'])\n",
    "\n",
    "# merge the two dataframes on the date column\n",
    "# asof merge\n",
    "# merged_data = pd.merge(wheat_data_full, daily_weighted, on='date', how='left')\n",
    "merged_data = pd.merge_asof(wheat_data_full, daily_weighted, on='date', direction='backward')\n",
    "\n",
    "# print the last 5 rows of the merged dataframe\n",
    "# print(merged_data.head())\n",
    "\n",
    "# save the merged dataframe to a csv file\n",
    "merged_data.dropna(inplace=True)\n",
    "merged_data.to_csv('data/data_for_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01    28624.069\n",
      "2024-04-01    29016.714\n",
      "2024-07-01    29374.914\n",
      "2024-10-01    29723.864\n",
      "2025-01-01    29962.047\n",
      "Name: GDP, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "gdp = fred.get_series('GDP')\n",
    "gdp.name = 'GDP'\n",
    "print(gdp.tail())\n",
    "fed_rate = fred.get_series('DFF')\n",
    "fed_rate.name = 'Fed_Rate'\n",
    "# print(fed_rate)\n",
    "# print(fed_rate.loc['2010-01-01':'2020-12-31'])\n",
    "# print(fed_rate.nunique())\n",
    "# print(fed_rate.index.max())\n",
    "cpi = fred.get_series('CPIAUCNS')\n",
    "cpi.name = 'CPI'\n",
    "# print(cpi.tail())\n",
    "\n",
    "# # Convert to DataFrames with 'date' column\n",
    "gdp = gdp.reset_index().rename(columns={'index': 'date'})\n",
    "fed_rate = fed_rate.reset_index().rename(columns={'index': 'date'})\n",
    "cpi = cpi.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# # # merge the two dataframes on the date column\n",
    "# merged_data = pd.merge(fed_rate, gdp, on='date', how='left')\n",
    "# merged_data = pd.merge(merged_data, cpi, on='date', how='left')\n",
    "# # ffill the merged data\n",
    "# merged_data = merged_data.ffill()\n",
    "# print(merged_data.tail())\n",
    "\n",
    "# # save the merged dataframe to a csv file\n",
    "# merged_data.to_csv('data/macro_data.csv', index=False)\n",
    "\n",
    "# save the merged dataframe to a csv file\n",
    "# merged_data.to_csv('data/merged_data.csv', index=False)\n",
    "\n",
    "# NOTE: save file as macro_data.csv\n",
    "merged_data = pd.merge_asof(fed_rate, gdp, on='date', direction='backward')\n",
    "merged_data = pd.merge_asof(merged_data, cpi, on='date', direction='backward')\n",
    "merged_data.to_csv('data/macro_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date   close    high     low    open  volume  weighted_tavg  \\\n",
      "6228 2025-05-22  544.50  552.00  541.25  548.25   85214      15.694328   \n",
      "6229 2025-05-23  542.50  548.50  538.50  545.00   61665      15.694328   \n",
      "6230 2025-05-27  528.50  544.00  527.50  543.50   66898      15.694328   \n",
      "6231 2025-05-28  530.25  535.75  526.75  531.00   60152      15.694328   \n",
      "6232 2025-05-29  534.00  534.75  527.25  533.00   48247      15.694328   \n",
      "\n",
      "      weighted_prcp  Fed_Rate      CPI        GDP  \n",
      "6228       1.365375      4.33  321.465  29962.047  \n",
      "6229       1.365375      4.33  321.465  29962.047  \n",
      "6230       1.365375      4.33  321.465  29962.047  \n",
      "6231       1.365375      4.33  321.465  29962.047  \n",
      "6232       1.365375      4.33  321.465  29962.047  \n",
      "           date   close    high     low    open  volume  weighted_tavg  \\\n",
      "6228 2025-05-22  544.50  552.00  541.25  548.25   85214      15.694328   \n",
      "6229 2025-05-23  542.50  548.50  538.50  545.00   61665      15.694328   \n",
      "6230 2025-05-27  528.50  544.00  527.50  543.50   66898      15.694328   \n",
      "6231 2025-05-28  530.25  535.75  526.75  531.00   60152      15.694328   \n",
      "6232 2025-05-29  534.00  534.75  527.25  533.00   48247      15.694328   \n",
      "\n",
      "      weighted_prcp  Fed_Rate      CPI        GDP  gk_vol_1d  gk_vol_21d  \n",
      "6228       1.365375      4.33  321.465  29962.047   0.013236    0.008798  \n",
      "6229       1.365375      4.33  321.465  29962.047   0.012693    0.009004  \n",
      "6230       1.365375      4.33  321.465  29962.047   0.013106    0.009169  \n",
      "6231       1.365375      4.33  321.465  29962.047   0.011947    0.008903  \n",
      "6232       1.365375      4.33  321.465  29962.047   0.009919    0.008600  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poyaohuang/Documents/Practice Program/Thesis/Agri_Forecast/venv/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# merge them asof backward one by one, start with daily, then fed rate, then cpi, then gdp\n",
    "df_wheat = pd.read_csv('data/wheat_data.csv')\n",
    "\n",
    "df_weather = pd.read_csv('data/daily_weighted.csv')\n",
    "\n",
    "fed_rate = fred.get_series('DFF')\n",
    "fed_rate.name = 'Fed_Rate'\n",
    "\n",
    "cpi = fred.get_series('CPIAUCNS')\n",
    "cpi.name = 'CPI'\n",
    "\n",
    "gdp = fred.get_series('GDP')\n",
    "gdp.name = 'GDP'\n",
    "\n",
    "# Convert to DataFrames with 'date' column\n",
    "gdp = gdp.reset_index().rename(columns={'index': 'date'})\n",
    "fed_rate = fed_rate.reset_index().rename(columns={'index': 'date'})\n",
    "cpi = cpi.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "# Rename columns\n",
    "df_wheat.rename(columns={'Date': 'date', 'Close': 'close', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "# Convert to datetime\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df_wheat['date'] = pd.to_datetime(df_wheat['date'])\n",
    "cpi['date'] = pd.to_datetime(cpi['date'])\n",
    "fed_rate['date'] = pd.to_datetime(fed_rate['date'])\n",
    "gdp['date'] = pd.to_datetime(gdp['date'])\n",
    "\n",
    "\n",
    "df_merged = pd.merge_asof(df_wheat, df_weather, on='date', direction='backward')\n",
    "\n",
    "df_merged = pd.merge_asof(df_merged, fed_rate, on='date', direction='backward')\n",
    "\n",
    "df_merged = pd.merge_asof(df_merged, cpi, on='date', direction='backward')\n",
    "\n",
    "df_merged = pd.merge_asof(df_merged, gdp, on='date', direction='backward')\n",
    "\n",
    "\n",
    "# rename Date column from wheat_data to date\n",
    "df_wheat.rename(columns={'Date': 'date'}, inplace=True)\n",
    "# df_macro['date'] = pd.to_datetime(df_macro['date'])\n",
    "\n",
    "df_merged.dropna(inplace=True)\n",
    "# print the last 5 rows of the dataframe\n",
    "print(df_merged.tail())\n",
    "# save the merged dataframe to a csv file\n",
    "df_merged.to_csv('data/merged_data.csv', index=False)\n",
    "\n",
    "# Avoid division by zero and log of negative numbers\n",
    "df_merged['high'] = np.maximum(df_merged['high'], df_merged['low'] + 1e-8)  # Ensure high > low\n",
    "df_merged['open'] = np.maximum(df_merged['open'], 1e-8)  # Ensure open > 0\n",
    "df_merged['close'] = np.maximum(df_merged['close'], 1e-8)  # Ensure close > 0\n",
    "\n",
    "# Calculate log returns\n",
    "log_hl = np.log(df_merged['high'] / df_merged['low'])\n",
    "log_co = np.log(df_merged['close'] / df_merged['open'])\n",
    "# Calculate realized volatility\n",
    "df_merged['gk_vol_1d'] = np.sqrt(0.5 * log_hl**2 - (2 * np.log(2) - 1) * log_co**2)\n",
    "df_merged['gk_vol_21d'] = df_merged['gk_vol_1d'].rolling(window=21).mean()\n",
    "df_merged.dropna(inplace=True)\n",
    "\n",
    "# print the last 5 rows of the dataframe\n",
    "print(df_merged.tail())\n",
    "# save the merged dataframe to a csv file\n",
    "df_merged.to_csv('data/merged_data_with_realised_volatility.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
